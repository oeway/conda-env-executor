# Conda Environment Executor

A Python package for executing code in isolated conda environments with efficient data passing between environments.

## Features

- Execute Python code in isolated conda environments
- Efficient data passing between environments using shared memory
- Support for conda-pack environments (portable tar.gz archives generated by [conda-pack](https://conda.github.io/conda-pack/))
- Environment caching for faster startup
- YAML-based environment specification
- Support for temporary environments
- Process safety with proper locking

## Installation

### Setting up the environment

First, create a Python 3.11 environment using conda or mamba:

```bash
# Using conda
conda create -n conda-env-executor python=3.11

# Or using mamba
mamba create -n conda-env-executor python=3.11

# Activate the environment
conda activate conda-env-executor
```

Then install the package:

```bash
pip install conda-env-executor
```

### Development Installation

For development, clone the repository and install with development dependencies:

```bash
git clone https://github.com/yourusername/conda-env-executor.git
cd conda-env-executor

# Install with development dependencies
pip install -e ".[dev,test]"
```

## Usage

### Getting Started

The simplest way to get started with Conda Environment Executor is to use temporary environments or YAML specifications:

#### Option 1: Using Temporary Environments

For quick testing or one-off executions, you can create temporary environments on-the-fly:

```python
from conda_env_executor import CondaEnvExecutor

# Create a temporary environment with specific packages
executor = CondaEnvExecutor.create_temp_env(['python=3.11', 'numpy', 'pandas'])

# Define code to run in the environment
code = """
import numpy as np
import pandas as pd

def process_data(data):
    return pd.DataFrame(data).describe().to_dict()

result = process_data(input_data)
"""

# Prepare input data
input_data = {"values": [1, 2, 3, 4, 5]}

# Execute code in the temporary environment
with executor:
    result = executor.execute(code, input_data)

print(result)
```

The temporary environment will be automatically created and cleaned up when no longer needed.

#### Option 2: Using Environment Specifications

For more reproducible workflows, define your environment in a YAML file:

```yaml
# environment.yaml
name: myenv
channels:
  - conda-forge
dependencies:
  - python=3.11
  - numpy
  - pandas
```

Then use it with the executor:

```python
from conda_env_executor import CondaEnvExecutor

executor = CondaEnvExecutor.from_yaml('environment.yaml')
with executor:
    result = executor.execute(code, input_data)
```

This approach creates and manages the environment for you without requiring manual packaging steps.

### Advanced Usage: Using Conda-Pack Environments

For production deployments or environments that need to be shared across machines, you can use [conda-pack](https://conda.github.io/conda-pack/) to create portable environment archives.

#### Packaging Environments with Conda-Pack

First, create a conda environment and package it:

```bash
# Create a conda environment
conda create -n myenv python=3.11 numpy pandas

# Activate the environment and install any additional packages
conda activate myenv
conda install scikit-learn matplotlib

# Package the environment into a portable archive using conda-pack
conda pack -n myenv -o myenv.tar.gz
```

Then use the packaged environment with the executor:

```python
from conda_env_executor import CondaEnvExecutor

# Create an executor using the packed environment
executor = CondaEnvExecutor("myenv.tar.gz")

# Define some code to run in the environment
code = """
import numpy as np
import pandas as pd

def process_data(data):
    df = pd.DataFrame(data)
    return df.describe().to_dict()

result = process_data(input_data)
"""

# Prepare input data
input_data = {"values": [1, 2, 3, 4, 5]}

# Execute the code in the isolated environment
with executor:
    result = executor.execute(code, input_data)

print(result)
```

Benefits of using conda-pack:
- Create once, deploy many times
- No internet connection needed for deployment
- Exact package versions preserved
- Faster startup times for complex environments
- Ideal for production or shared environments

## Conda-Pack Tutorial

[Conda-pack](https://conda.github.io/conda-pack/) is a tool for creating relocatable conda environments. This is useful for deploying code in an isolated environment, copying environments to a different location or machine, or for archiving environments.

### 1. Install conda-pack

First, you need to install conda-pack:

```bash
# Install in your base environment
pip install conda-pack

# Or install in a specific environment
conda install -c conda-forge conda-pack
```

### 2. Create and Set Up Your Environment

Create a conda environment with the packages you need:

```bash
# Create a new environment
conda create -n myenv python=3.11 numpy pandas scikit-learn matplotlib

# Activate the environment
conda activate myenv

# Install any additional packages
pip install some-package
```

### 3. Package Your Environment

Once your environment is set up with all required packages, use conda-pack to create a portable archive:

```bash
# Basic packaging (from outside the environment)
conda pack -n myenv -o myenv.tar.gz

# Or if you're inside the environment
conda pack -o myenv.tar.gz

# For more verbose output
conda pack -n myenv -o myenv.tar.gz --verbose
```

### 4. Using the Packed Environment

There are multiple ways to use the packed environment:

#### Option A: Unpacking for direct use (on the same architecture)

```bash
# Create a directory for the environment
mkdir -p /path/to/extract/myenv

# Extract the environment
tar -xzf myenv.tar.gz -C /path/to/extract/myenv

# Activate the environment
source /path/to/extract/myenv/bin/activate

# When you're done
source /path/to/extract/myenv/bin/deactivate
```

#### Option B: Using with conda-env-executor (this package)

Use the packed environment with conda-env-executor as shown in the Basic Usage section above.

### 5. Troubleshooting Conda Packs

If you encounter issues with your packed environment:

- Make sure all dependencies are properly installed in the original environment
- Try packaging with `--ignore-editable` if you have editable packages
- Use `--ignore-missing-files` if there are path conflicts
- For compatibility across different systems, pack from a similar OS/architecture as the target system

### 6. Cleaning Up

After packaging, you can clean up temporary files created during packaging:

```bash
# Clean up the prefixes directory in the original environment
conda pack -n myenv --clean
```

## Requirements

- Python >=3.10
- pyyaml >=6.0
- psutil >=5.9.0
- conda-pack >=0.7.0

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

For development, you'll need additional dependencies that can be installed with:
```bash
pip install -e ".[dev,test]"
```

This will install:
- Testing: pytest, pytest-cov, numpy
- Development: black, isort, mypy

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

This project incorporates ideas and code from:
- [conda-execute](https://github.com/conda-tools/conda-execute) (BSD 3-Clause License)
- [conda-pack](https://github.com/conda/conda-pack) (BSD 3-Clause License)

# SandboxAI Evaluation

This repository contains a test script to evaluate the capabilities of SandboxAI, particularly focusing on:
- Basic sandbox functionality
- Package installation
- Image analysis using scikit-image
- Machine learning using scikit-learn
- Plotting capabilities with matplotlib

## Setup

1. Install the required dependencies:
```bash
pip install -r requirements.txt
```

2. Run the test script:
```bash
python sandbox_test.py
```

## Test Cases

The script includes three main test cases:

1. **Basic Sandbox Test**: Tests basic Python execution and package installation within the sandbox.

2. **Image Analysis Test**: Demonstrates image processing capabilities using scikit-image:
   - Loads a sample image
   - Converts to grayscale
   - Applies Gaussian blur
   - Calculates image statistics
   - Generates comparison plots (saved as 'image_analysis.png')

3. **Machine Learning Test**: Shows machine learning capabilities using scikit-learn:
   - Creates a synthetic classification dataset
   - Trains a Random Forest classifier
   - Evaluates model performance
   - Plots feature importance (saved as 'feature_importance.png')

## Output

The script will generate:
- Console output with test results
- Two image files:
  - `image_analysis.png`: Showing image processing results
  - `feature_importance.png`: Showing feature importance from the ML model

# Docker-based Python Sandbox

This repository contains a test script that demonstrates how to create isolated Python environments using Docker containers. The implementation focuses on:
- Running Python code in isolated containers
- Installing and using various Python packages
- Performing image analysis and machine learning tasks
- Generating and saving plots and results

## Requirements

1. Docker must be installed and running on your system
2. Python 3.6+ with pip

## Setup

1. Install the required Python dependencies:
```bash
pip install -r requirements.txt
```

2. Make sure Docker daemon is running

3. Run the test script:
```bash
python docker_sandbox_test.py
```

## Test Cases

The script includes three main test cases:

1. **Basic Sandbox Test**: Tests basic Python code execution in an isolated container.

2. **Image Analysis Test**: Demonstrates image processing capabilities:
   - Loads a sample image from scikit-image
   - Converts to grayscale
   - Applies Gaussian blur
   - Calculates image statistics
   - Generates comparison plots (saved as 'image_analysis.png')

3. **Machine Learning Test**: Shows machine learning capabilities:
   - Creates a synthetic classification dataset
   - Trains a Random Forest classifier
   - Evaluates model performance
   - Plots feature importance (saved as 'feature_importance.png')

## How it Works

The `DockerSandbox` class provides the following functionality:
- Creates temporary directories for code execution
- Builds custom Docker images with required dependencies
- Runs code in isolated containers
- Captures output and saves generated files
- Automatically cleans up containers and temporary files

## Output

The script generates:
- Console output with test results
- Two image files in the temporary directory:
  - `image_analysis.png`: Showing image processing results
  - `feature_importance.png`: Showing feature importance from the ML model

## Security Notes

This implementation provides isolation through Docker containers, which offers several benefits:
- Isolated filesystem
- Controlled resource usage
- Clean environment for each execution
- Safe package installation and management